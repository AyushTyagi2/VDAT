(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[5602],{6965:(e,n,t)=>{Promise.resolve().then(t.bind(t,639))},639:(e,n,t)=>{"use strict";t.r(n),t.d(n,{default:()=>s});var a=t(5155);t(2115);var r=t(4055),i=t(8370),o=t(230);let s=()=>(0,a.jsxs)("div",{className:"bg-gray-100 min-h-screen flex flex-col",children:[(0,a.jsx)("div",{className:"bg-black",children:(0,a.jsx)(i.default,{})}),(0,a.jsx)(o.A,{title:"Google's Tensor Processing Unit: Understanding State-of-the-art AI Accelerator",duration:"90 minutes",speakers:["Sparsh Mittal"],format:"Presentation",abstract:"Computing systems have fueled the growth of artificial intelligence (AI). Improvements in AI\nalgorithms have inevitably gone hand-in-hand with the improvements in the hardware\naccelerators. Our ability to train increasingly complex AI models and achieve low-power,\nreal-time inference depends on the capabilities of computing systems.\n\nIn recent years, the metrics used for optimizing and evaluating AI algorithms are diversifying:\nalong with accuracy, there is increasing emphasis on the metrics such as energy efficiency\nand model size. Given this, researchers working on AI can no longer afford to ignore the\ncomputing system. Instead, the knowledge of the potential and limitations of computing\nsystems can provide invaluable guidance to them in designing the most efficient and\naccurate algorithms.\n\nThis tutorial seeks to arouse curiosity and even an interest in the AI accelerators, with the\nexample of one of the most popular commercial accelerator, viz., Google's TPU. We first\npresent the basics, viz., systolic array architecture for matrix multiplication. Then, we dive\ninto the architecture of TPU, its salient features, comparison with CPU and GPU architecture,\nand evaluation results on AI workloads. We finally provide a view of evolution of Google's\nTPU over 6 versions to learn how TPU has transformed to address the needs of changing AI\nworkloads.\n\nThis tutorial is at the intersection of deep learning algorithms, computer architecture, and\nchip design, and thus, is expected to be beneficial for a broad range of learners.",domain:"This falls at the intersection of computer architecture and artificial intelligence.",keywords:"Tensor Processing Unit (TPU), Computer Architecture, Artificial Intelligence, Hardware Accelerator, Systolic Array",learningOutcomes:["a) Understanding the importance and potential of hardware accelerators for AIb) Understanding the architecture and salient features ofGoogle's Tensor Processing Unitc) Understanding the relative comparison of architectures ofgeneral-purpose (CPU/GPU) and custom accelerators"],targetAudience:"This tutorial will be relevant to engineering students, industry professionals working in the area of computer architecture, electronics and AI.",prerequisites:"No Prerequisites. Open to all.",speakerBiographies:["\nDr. Sparsh Mittal is currently working as an associate professor in the ECE department at IIT\nRoorkee. He is also a joint faculty at Mehta Family School of Data Science and Artificial\nIntelligence at IIT Roorkee. He received the B.Tech. degree from IIT, Roorkee, and the Ph.D.\ndegree from Iowa State University (ISU), USA. He has worked as a Post-Doctoral Research\nAssociate at Oak Ridge National Lab (ORNL), USA. He has received best paper awards or\nhonorable mentions at AIMLSystems, VLSID and ICPR conferences. In 2024, he was\nshortlisted in the list of top-5 faculty members in IIT Roorkee for the excellence in teaching\naward based on students' feedback. His research has been funded by SERB and\nSemiconductor Research Corporation (USA). He has served as a reviewer, TPC member or\ntrack chair at prestigious conferences and journals. He has published more than 130 papers\nat top venues and has nearly 9500 citations. His research interests are AI for computer\nvision, applications of AI, hardware security and computer architecture. His webpage is\nhttp://faculty.iitr.ac.in/~sparshfec/."],basicStructure:"The tutorial titled:\n“Architectural Insights into Google’s Tensor Processing Unit (TPU)”\nwill be highly relevant to students, researchers, and professionals working in the area of AI hardware, computer architecture, and accelerator design.\n\n1. Introduction to Systolic Array (15 Mins)\n– Concept and working of systolic arrays\n– Historical background and evolution\n– Importance of systolic architecture in deep learning workloads\n\n2. Architecture of Google’s Tensor Processing Unit (20 Mins)\n– Internal architecture of TPU\n– Matrix Multiply Unit (MXU) and systolic data flow\n– On-chip memory organization and interconnects\n\n3. Salient Features of TPU and Comparison with CPU/GPU (20 Mins) \n– Key design features of TPU\n– Comparison of TPU with traditional CPU and GPU architectures\n– Application-specific advantages and limitations\n\n4. Evaluation Results of TPU (15 Mins) \n– Performance benchmarking metrics\n– Power efficiency and throughput evaluation\n– Observations from real-world deployments\n\n5. Architectural Evolution of TPU (20 Mins) \n– Evolution from TPU v1 to TPU v6\n– Improvements in architecture, scalability, and AI support\n– Use cases and design motivations across versions",youtubeVideoId:"1Ur7ozXrS2Sw5DsSufq4lY_xhHX4uMarq",videoId:"1Ur7ozXrS2Sw5DsSufq4lY_xhHX4uMarq",videoType:"gdrive"}),(0,a.jsx)(r.A,{})]})}},e=>{var n=n=>e(e.s=n);e.O(0,[6711,6851,9219,5547,1028,230,8441,1517,7358],()=>n(6965)),_N_E=e.O()}]);